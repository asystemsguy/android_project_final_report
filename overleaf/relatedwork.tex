%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\vspace{0.1in}
\section{Related work}
\label{related}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Existing solutions for service discovery}

There are multiple solutions for service discovery in microservices. The simplest solution to registration and discovery is to just put all of the service endpoints belonging to a microservice behind a single DNS name ~\cite{cheshire2013dns}. To address a service, we can contact it by DNS name and the request should get to a random back-end hosting the microservice. Main drawbacks of this approach are DNS suffers from propagation delays; even after a server failure is detected a de-registration command issued to DNS, there will be at least a few seconds before this information gets to the consumers. Also, due to the various layers of caching in the DNS infrastructure, the exact propagation delay is often non-deterministic. Another major problem is that a service is identified just by name, there is no way to determine which boxes get traffic. We will get the equivalent of random routing, with loads chaotically piling up behind some back-ends while others are left idle. 

In contrast to DNS based approach, our NDN based service discovery can load balance the flows ~\cite{tan2016flow} among different instances of the same microservice at the NDN router. It has faster recovery from service failures. As soon as NDN router identify service failures; it will invalidate the link. if a new request comes to the same service our NDN router will simply use alternate path for propagation. 

Etcd ~\cite{etcd} is a key-value store that provides shared configuration and service discovery for Container Linux clusters. etcd runs on each node trying to form a cluster using Raft ~\cite{raft} with the others for achieving high availability and fault-tolerance. It also supports service reconfiguration by watching updates to key prefixes with the service name. Another work is Consul ~\cite{Consul} that provide consistent key-value store to deliver service discovery and integrated health checking based Consul agents. These agents communicate with Consul servers where data is stored and replicated, to maintain cluster state. Synapse ~\cite{Synapse} is a system for service discovery
whose heart is a HAProxy, a stable and proven routing component. Synapse runs the HAProxy on application servers that are used to route requests from application servers to service providers running in the cluster. In order to provide service discovery, Synapse comes with several watchers, which frequently check for changes to the service location to update the
HAProxy configuration. Docker Swarm ~\cite{swarm} is a native cluster system for Docker hosts. Docker Swarm decides where service images should be deployed to, thus each Swarm needs a key-value store which acts as a DNS server to store the location of each service. Each host has a Swarm agent running which is responsible for advertising its attributes to the key-value store.

Above-mentioned solutions use a consistent key-value store that is mainly achieved through cluster implementation. In practice, service discovery mechanisms that use the cluster to preserve the system reliability adds complexity to deployments. With the solutions that form a quorum to maintain clusterâ€™s activities strongly depend on the number of servers. Without a sufficient number of cluster nodes, the server nodes cannot
form a Raft-based quorum, thus the cluster cannot operate, and data loss can occur. Moreover, these solutions might occur high event advertising overhead for synchronizing the service state on every cluster nodes. Furthermore, there would be a significant increment of service records due to the number of available service instances. Therefore, it is very difficult to scale when applications have larger service popularity or
applications grow larger.
